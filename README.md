Hereâ€™s a sample `README.md` for a project focused on analyzing the sample web server log data you provided:

---

# ğŸ“Š WebLog Analyzer

## Project Overview

**WebLog Analyzer** is a Python-based project designed to parse, process, and analyze raw HTTP server logs in the common log format (CLF). This repository helps uncover usage patterns, diagnose issues, and extract insights from access logs generated by web servers.

The provided sample reflects real-world usage from a site, including assets served, repeated requests, HTTP status codes, and IP activity â€” ideal for performance analysis and anomaly detection.

---

## ğŸ“ Dataset

### Sample Log Entry Format

```
10.223.157.186 - - [15/Jul/2009:14:58:59 -0700] "GET / HTTP/1.1" 403 202
```

Each line includes:

* **IP address** of the client
* **Timestamp** of the request
* **HTTP method** and resource requested
* **HTTP response code**
* **Bytes transferred**

---

## ğŸ” Features

* ğŸ”¢ **Request Frequency Analysis**

  * Most requested endpoints
  * Request distribution by hour/day
* ğŸ§¾ **Status Code Breakdown**

  * Frequency of 200, 404, 403, 304, etc.
* ğŸ§‘â€ğŸ’» **Client IP Insights**

  * Top visitors by IP
  * Request patterns per client
* ğŸ“ **Resource Type Analysis**

  * Static assets (.js, .css, .jpg, .gif)
  * Media usage patterns
* â° **Temporal Trends**

  * Request peaks
  * Time-series analysis

## Installation

Create a Python virtual environment using [uv](https://github.com/astral-sh/uv) a
nd install the dependencies:

```bash
# create and activate the environment
uv venv .venv
source .venv/bin/activate

# install packages
uv pip install -r requirements.txt
```

## Running the Parser

Execute the log parser with PySpark to display the parsed fields:

```bash
python parse_logs_spark.py access_log_sample.txt
```

## Running the Unit Tests

After installing the dependencies, run the test suite with:

```bash
pytest
```

